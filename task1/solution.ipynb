{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from course_intro_ocr_t1.data import MidvPackage\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path('midv500_compressed').resolve()\n",
    "assert DATASET_PATH.exists(), DATASET_PATH.absolute()\n",
    "\n",
    "packs = MidvPackage.read_midv500_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    def __init__(self, datapacks, is_test):\n",
    "        self.datapacks = datapacks\n",
    "        self.data_indexes = []\n",
    "        \n",
    "        for i in range(len(datapacks)):\n",
    "            for j in range(len(datapacks[i])):\n",
    "                if datapacks[i][j].is_test_split() == is_test:\n",
    "                    self.data_indexes.append((i, j))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_indexes)\n",
    "\n",
    "    def __getitem__(self, idx):       \n",
    "        i, j = self.data_indexes[idx]\n",
    "        \n",
    "        image = torch.FloatTensor(np.array(self.datapacks[i][j].image.convert('RGB'))) / 255\n",
    "        image = image.permute([2, 0, 1])\n",
    "        image = transforms.Resize((256, 256))(image)\n",
    "\n",
    "        trg = cv.fillConvexPoly(np.zeros(np.array(self.datapacks[i][j].image).shape[:2]), np.array(self.datapacks[i][j].gt_data['quad']), 1)\n",
    "        trg = transforms.ToTensor()(trg)\n",
    "        trg = transforms.Resize((256, 256), interpolation=Image.NEAREST)(trg)\n",
    "\n",
    "        return image, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CropDataset(packs, is_test=False)\n",
    "val_set = CropDataset(packs, is_test=True)\n",
    "\n",
    "batch_size = 6\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_set, batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out += residual\n",
    "        out = nn.ReLU()(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_conv1 = ResNetBlock(in_channels, 32)\n",
    "        self.enc_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv2 = ResNetBlock(32, 64)\n",
    "        self.enc_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv3 = ResNetBlock(64, 128)\n",
    "        self.enc_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv4 = ResNetBlock(128, 256)\n",
    "        self.enc_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.bridge = ResNetBlock(256, 512)\n",
    "        \n",
    "        self.dec_upsample1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(512, 256, kernel_size=1)\n",
    "        )\n",
    "        self.dec_conv1 = ResNetBlock(512, 256)\n",
    "\n",
    "        self.dec_upsample2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(256, 128, kernel_size=1)\n",
    "        )\n",
    "        self.dec_conv2 = ResNetBlock(256, 128)\n",
    "\n",
    "        self.dec_upsample3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(128, 64, kernel_size=1)\n",
    "        )\n",
    "        self.dec_conv3 = ResNetBlock(128, 64)\n",
    "\n",
    "        self.dec_upsample4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 32, kernel_size=1)\n",
    "        )\n",
    "        self.dec_conv4 = ResNetBlock(64, 32)\n",
    "        \n",
    "        self.out = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc_conv1(x)\n",
    "        enc2 = self.enc_conv2(self.enc_pool1(enc1))\n",
    "        enc3 = self.enc_conv3(self.enc_pool2(enc2))\n",
    "        enc4 = self.enc_conv4(self.enc_pool3(enc3))\n",
    "        \n",
    "        bridge = self.bridge(self.enc_pool4(enc4))\n",
    "        \n",
    "        out = self.dec_upsample1(bridge)\n",
    "        out = torch.cat([out, enc4], dim=1)\n",
    "        out = self.dec_conv1(out)\n",
    "        \n",
    "        out = self.dec_upsample2(out)\n",
    "        out = torch.cat([out, enc3], dim=1)\n",
    "        out = self.dec_conv2(out)\n",
    "\n",
    "        out = self.dec_upsample3(out)\n",
    "        out = torch.cat([out, enc2], dim=1)\n",
    "        out = self.dec_conv3(out)\n",
    "        \n",
    "        out = self.dec_upsample4(out)\n",
    "        out = torch.cat([out, enc1], dim=1)\n",
    "        out = self.dec_conv4(out)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitIdentifier(lightning.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.model.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | UNet              | 7.6 M \n",
      "1 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "7.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.6 M     Total params\n",
      "30.386    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1089ff6c709d46fa8638864d7b4ff5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10abe6d94af4760ae3894d5682ab7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfcca370c8e4b4a856aa1ddf3ad4b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca3adeddc24433e9a36e4af18fcd7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3157e75e913f4ef18f2d60f4e2ad268a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc5ea9de35347949325d52083c2fc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc89a54f87a491ab3de9809b9c687d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "\n",
    "lit_model = LitIdentifier(model)\n",
    "trainer = lightning.Trainer(max_epochs=5)\n",
    "trainer.fit(lit_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corners(mask):\n",
    "    mask = (torch.sigmoid(mask) > 0.2).cpu().numpy().astype(np.uint8)\n",
    "    contours, hierarchy = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contour = max(contours, key=cv.contourArea)\n",
    "    x, y = np.array(contour)[:, 0].T\n",
    "    \n",
    "    right_bottom = np.argmax(y + x)\n",
    "    left_bottom = np.argmax(y - x)\n",
    "    left_top = np.argmax(-y - x)\n",
    "    right_top = np.argmax(-y + x)\n",
    "\n",
    "    corners = np.array([[x[left_top], y[left_top]], \n",
    "                        [x[right_top], y[right_top]], \n",
    "                        [x[right_bottom], y[right_bottom]], \n",
    "                        [x[left_bottom], y[left_bottom]]], dtype=float)\n",
    "    \n",
    "    return corners\n",
    "\n",
    "def to_relative_scale(corners, height, width):\n",
    "    for i in range(len(corners)):\n",
    "        corners[i][0] /= width\n",
    "        corners[i][1] /= height\n",
    "    return corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = dict()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for dp in packs:\n",
    "    for i in range(len(dp)):\n",
    "        if dp[i].is_test_split():\n",
    "            try:\n",
    "                image = torch.FloatTensor(np.array(dp[i].image.convert('RGB'))) / 255\n",
    "                image = image.permute([2, 0, 1])\n",
    "                image = transforms.Resize((256, 256))(image)\n",
    "                mask = model(image.unsqueeze(0).to(device))\n",
    "                corners = get_corners(mask.squeeze()).reshape(-1, 2)\n",
    "                results_dict[dp[i].unique_key] = to_relative_scale(np.array(corners, dtype=np.float32), mask.size()[2], mask.size()[3])\n",
    "            except Exception as exc:\n",
    "                # Для пропущенных в словаре ключей в метриках автоаматически засчитается IoU=0\n",
    "                print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность кропа: 0.8125\n"
     ]
    }
   ],
   "source": [
    "from course_intro_ocr_t1.metrics import dump_results_dict, measure_crop_accuracy\n",
    "\n",
    "dump_results_dict(results_dict, Path() / 'pred.json')\n",
    "\n",
    "acc = measure_crop_accuracy(\n",
    "    Path() / 'pred.json',\n",
    "    Path() / 'gt.json'\n",
    ")\n",
    "\n",
    "print(\"Точность кропа: {:1.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'UNet.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
